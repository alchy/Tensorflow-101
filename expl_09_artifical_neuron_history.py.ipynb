{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who is developing neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one hand, although there are many analogies between the basic concepts of neurophysiology and the neural-network models, we caution you not to portray these systems as actually modeling the brain. We prefer to say that these networks have been inspired by our current understanding of neurophysiology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is a very brief summary of the history of neural networks, in terms of the developlent of architectures and algorithms that are widely used today.Biological research serve as the inspiration for a number of networks that are applicable to problems beyond the original ones studied. The history of neural network shwols the interplay among biological experimentation, modeling and computer simulation/hardware implementation. Thus, the field is strongly interdisciplinary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1940: začátek neuronových sítí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>McCulloch&Pitts neurons</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warren McCulloch and Walter Pitts designed what are generally reagarded as the first neural networks [McCulloch & Pitts, 1943]. These researched recognized that combining many simple neurons into neural systems was source of increased computational power. The weights on McCulloch&Pitts neurons are set so that the neuron performs a particular simple logic function, with different neurons performing different functions. The neurons can be arranged into a net to produce any output that can be represented as a combination of logic functions. The flow of information through the net assumes a unit time step for a signal travel from one neuron to the next. This time delay allows net to model some physiological processes as the perception of hot and cold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of a threshold such that if the net input to neuron is greater than the treshold then then the unit fires is one of the features of a McCulloch-Pitts neuron that is used in many artifical neurons today. However, McCulloch-Pitts neurons are used most widely as logic circuits [Anderson & Rosendfeld, 1988]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McCulloch-Pitts model, where symbolic logic was employed to analyze rather idealized structures. This work is important for many reasons, not the least of which is that the investigators were the first people to treat the brain as a computational organism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McCulloch and Pitts subsequent work [Pitts & McCulloch, 1947] addressed issues that are still important research areas today, such as translation and rotation invariant pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the McCulloch-Pitts theory has turned out not to be an accurate model of brain activity, the importance of the work cannot be overstated. The theory helped to shape the thinking of many people who were influential in the development of modern computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The activity of a neuron is an all-or-none process.\n",
    "2. A certain fixed number of synapses (> 1) must be excited within a period of latent addition for a neuron to be excited.\n",
    "3. The only significant delay within the nervous system is synaptic delay.\n",
    "4. The activity of any inhibitory synapse absolutely prevents excitation of the neuron at that time.\n",
    "5. The structure of the interconnection network does not change with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hebb learning</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Hebb, a psychologist at McGill University, designed the first learning law for artifical neural networks [Hebb, 1949]. His premise was that if two neurons were active simultaneously, then the strenght of the connection between them should be increased. Refinements were subsequently made to this rather general statement to allow computer simulations [Rochester, Holland, Haibt & Duda, 1956]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1950 a 1960: první zlatá éra neuronových sítí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>John von Neumann</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although today neural networks are often viewed as an alternative to (or complement of) traditional computing, it is interesting to note that John von Neumann, the 'father of modern computing', was keenly interested in modelling the brain [von Neumann, 1958] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perceptrons</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Togehter with several other researchers [Block, 1962; Minsky & Papert, 1969], Frank Rosenblatt (1958, 1959, 1962) introduced and developed a large class of artifical neural networks called perceptrons. The most typical perceptron consisted of an input layer (the retina) connected by paths with fixed weights to assocoator neurons; the weights on the connection paths were adjustable. The Perceptron learning rule uses an interactive weight adjustment that is more powerful than Hebb rule. Perceptron learning can be proved to converge to the correct weights if there are weights that will solve the problem at hand. Rosseblatt's 1962 work describes many types of perceptrons. Like the neurons developed by McCulloch and Pitts and by Hebb, perceptrons use a treshold output function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The early success with perceptrons led to enthusiastic claims. However, the mathematical proof of the convergence of iterative learning under suitable assumptions was followed by a demonstration of the limitations regarding what the perceptron type of net can learn [Minsky & Papert, 1969]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ADALINE</b> (Adaptive Linear Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernard Widrow and his student, Marcian (Ted) Hoff [Widrrow & Hoff, 1960], developed a learning rule (which ussualy either bears treir names, or is designated the least mean squares or delta rule) that is closely related to the perceptron learning rule. The perceptron rule adjusts the connection weights to a unit whenever the response of the unit is incorrect. (The response indicates a classification of the of the input pattern.) The delta rule adjusts the weights to reduce the difference between the net input to the output unit and the desired output. This result in the smallest mean squared error. The similarity of models developed in psychology by Rosenblatt to those developed in electrical engineering by Widrow nad Hoff is evidence of the interdisciplinary nature of neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in learning rules, although slight, leads to an improved ability of the net to generalize (i.e., respond to input that is similar, but not identical, to that, on which it was trained). The Widrow-Hoff learning rule for a single-layer network is precursor of the backpropagation rule for multilayer nets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work by Widrow and his student is sometimes reported as neural network research, sometimes as adaptive linear systems. The name ADALINE, interpreted as either Adaprive Linear Neuron or Adaptive Linear system, is often given to these nets. There have been many interesting applications of ADALINEs, from neural networks for adaptive antenna systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1970: tiché roky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of Minsky and Papert's demonstration of the limitations of perceptrons (i.e.) single-layer nets, research on neural networks continued. Many of the current leaders in the field began to publish their work during 1970 - Teuvo Kohonen, James Anderson, Stephen Grossberg, Gail Carpenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, perceptrons caused a fair amount of controversy at the time they were described. Unrealistic expectations and exaggerated claims no doubt played a part in this controversy. The end result was that the field of artificial neural networks was almost entirely abandoned, except by a few die-hard researchers. We hinted at one of the major problems with perceptrons when we suggested that there were conditions attached to the successful operation of the perceptron. In the next section, we explore and evaluate these considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1980: enthusiasmus se vrací"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Backpropagation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the reasons for the quiet years of the 1970 were the failure of single-layer perceptrons to be able to solve such simple problems (mappings) as the XOR function and thelack of general method training multilayer net. A method for propagating information about errors at the output unit back to the hidden units had been discovered in the previous decade [Werbos, 1974], but has not gained wide publicity. This method was also discovered independently by David Parker (1985) and LeCun (1986) before it became widely known. It is very similar to yet an earlier algorithm in optimal control theory [Bryson & Ho, 1969]. Parker's work came to the attention of the Parallel Distributed Processing Group led by psychologist David Rumelhart, of the Univerisy of California at San Diege, and James McClelland, of Carnegie-Mellon University, who refined and publicized it [Rumelhart, Hinton & Williams, 1986; McClelland & Rumelhart, 1988]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hopfield nets</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Neocognitron</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Boltzman machine</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hardware implementation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reason for renewed interest in neural networks is improved computational capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unrealistic expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron can differentiate patterns only if the patterns are linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons can differentiate patterns only if the patterns are linearly separable. The meaning of the term linearly separable should become clear shortly. Because many classification problems do not possess linearly separable classes, this condition places a severe restriction on the applicability of the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Počítačový model Neuronu popsaný tf Grafem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pictures/artificial_neuron.png \"https://inspirehep.net/record/1300728/plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Náš první perceptron bude mít tři vstupy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_inputs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    neuron_inputs = tf.placeholder(tf.float32, [nr_inputs, 1], name='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('neuron'):\n",
    "\n",
    "    with tf.name_scope('Weights'):\n",
    "        neuron_weights = tf.placeholder(tf.float32, [nr_inputs, 1], name='Weight')\n",
    "\n",
    "    with tf.name_scope('bias'):\n",
    "        neuron_bias = tf.placeholder(tf.float32, [1], name='bias')\n",
    "\n",
    "    with tf.name_scope('neuron_fn'):\n",
    "        neuron_fn1_out = tf.multiply(neuron_inputs, neuron_weights, name='Wx')\n",
    "        neuron_fn2 = tf.reduce_sum(neuron_fn1_out)\n",
    "        neuron_fn3 = tf.add(neuron_fn2, neuron_bias)\n",
    "        neuron_out = tf.sigmoid(neuron_fn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90720707]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"./tensorboard_example\", sess.graph)\n",
    "    neuron_inputs_feed =  [[0.8], [0.8], [0.8]]\n",
    "    neuron_weights_feed = [[0.1], [0.5], [1.0]]\n",
    "    neuron_bias_feed = [1.0]\n",
    "                            \n",
    "    res_neuron_out = sess.run(neuron_out,\n",
    "        feed_dict={neuron_inputs: neuron_inputs_feed,\n",
    "                   neuron_weights: neuron_weights_feed,\n",
    "                   neuron_bias: neuron_bias_feed})\n",
    "\n",
    "    print(res_neuron_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard.exe --logdir=\".\\tensorboard_example\" --port 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentals Of Neural Networks.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
